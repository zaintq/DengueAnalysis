# Zain Tariq

# I have chosen this code because I have recently used this script in one of my
# projects and it uses tools complex enough to give a decent idea of my skills 
# to the reviewer.

import numpy as np
import json
import itertools
import csv
import datetime
from db_conn import *
from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering
from sklearn import metrics
from sklearn.datasets.samples_generator import make_blobs
from sklearn.preprocessing import StandardScaler

from scipy.spatial import ConvexHull

import matplotlib.pyplot as plt

import webbrowser

from haversine import haversine

def getData(year = False):
    
    lyon = (45.7597, 4.8422)
    paris = (48.8567, 2.3508)
    print haversine(lyon, paris)
    
    db     = DB()
    coords = []
    if year:
        date = {"start": str(year)+"-08-01", "end": str(year)+"-12-31"}
        sql = {
            "patients": "SELECT lat, lng, created_at FROM irs_patients WHERE tag='irs_patient' AND district = 'Lahore' AND created_at BETWEEN '"+start_date+"' AND '"+end_date+"'",
            "larvae": "SELECT lat, lng, created_at FROM positive_larvae WHERE district = 'Lahore' AND created_at BETWEEN '"+start_date+"' AND '"+end_date+"'"
        }
    else:
        sql = {
            "patients":"SELECT lat, lng, created_at FROM irs_patients WHERE tag='irs_patient' AND district = 'Lahore'",
            "larvae":"SELECT lat, lng, created_at FROM positive_larvae WHERE district = 'Lahore'"
        }

    db.query(sql["patients"])
    for row in db.fetchall(): # float(row[0]), float(row[1]), row[2].date(), 'patient']
        coords.append([float(row[0]), float(row[1]), 'patient'])

    db.query(sql["larvae"])
    for row in db.fetchall():
        coords.append([float(row[0]), float(row[1]), 'larvae'])
   
    print coords
  
    return np.array(coords)

def removeDuplicates(X):
  b = np.ascontiguousarray(X).view(np.dtype((np.void, X.dtype.itemsize * X.shape[1])))
  _, idx = np.unique(b, return_index=True)
  return X[idx]

def xWithoutDate(X):
  return [[x[0], x[1]] for x in X]

def transformData(X): 
  return StandardScaler().fit_transform(xWithoutDate(X))

def plotDBSCAN(X, epsilon, minPts):
  db = DBSCAN(eps=epsilon, min_samples=minPts).fit(X)
  labels = db.labels_

  core_samples_mask = np.zeros_like(labels, dtype=bool)
  core_samples_mask[db.core_sample_indices_] = True
  
  n_clusters = len(set(labels)) - (1 if -1 in labels else 0)

  clusters = [X[labels == i] for i in xrange(n_clusters)]

  unique_labels = set(labels)
  colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))
  for k, col in zip(unique_labels, colors):
      if k == -1: col = 'k'
      class_member_mask = (labels == k)
      xy = X[class_member_mask & core_samples_mask]
      plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=14)
      xy = X[class_member_mask & ~core_samples_mask]
      plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=6)

  print('Estimated number of clusters: %d' % n_clusters)
  print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(X, labels))
  
  plt.title('Estimated number of clusters: %d' % n_clusters)

  return clusters

def plotKMeans(X, k):
  k_means = KMeans(n_clusters=k)
  k_means.fit(X)

  labels = k_means.labels_

  core_samples_mask = np.zeros_like(labels, dtype=bool)
  
  n_clusters = len(set(labels)) - (1 if -1 in labels else 0)

  clusters = [X[labels == i] for i in xrange(n_clusters)]

  # print "c=", n_clusters, "k=", k

  unique_labels = set(labels)
  colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))
  print k, "%0.3f" % metrics.silhouette_score(X, labels)
  for k, col in zip(unique_labels, colors):
      if k == -1: col = 'k'
      class_member_mask = (labels == k)
      xy = X[class_member_mask & core_samples_mask]
      plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=14)
      xy = X[class_member_mask & ~core_samples_mask]
      plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=6)

  # print('Estimated number of clusters: %d' % n_clusters)
  # print "Silhouette Coefficient: %0.3f" % metrics.silhouette_score(X, labels)
  
  
  plt.title('Estimated number of clusters: %d' % n_clusters)

  return clusters

# algo = "-DBSCAN-"
algo = "__KKMEANS__"

epsilon = 0.3
minPts = 2

k = 19

# for year in [2012]:

year = False

print 'Year:', year

data = getData(year)

# for k in range(2, 10):
# print('K: %d' % k)
X = transformData(data)

np.set_printoptions(suppress=True)
np.set_printoptions(threshold=np.inf)

data_dict = {}
for datum, x in itertools.izip_longest(data,X):
    data_dict[tuple(x)] = list(datum)

# clusters = plotDBSCAN(removeDuplicates(X), epsilon, minPts)
clusters = plotKMeans(removeDuplicates(X), k)

prefix = algo+str(k)

cluster_files = []
hull_files = []

for cluster, i in zip(clusters, range(len(clusters))):

  cluster_coords = []

  if len(cluster) > 2 :

    for coords in cluster:
      cluster_coords.append(data_dict[tuple(coords)])

    cluster_file = "%scluster%d.csv"%(prefix,i)
    cluster_files.append([cluster_file])
    csv.writer(open(cluster_file,"wb")).writerows(cluster_coords)
    np.savetxt("%scluster%d.csv"%(prefix,i), cluster_coords, delimiter=",")

    hull = ConvexHull(xWithoutDate(cluster_coords))
    hull_vertices = []

    for vertex in hull.vertices:
      hull_vertices.append(cluster_coords[vertex])

    hull_vertices.append(hull_vertices[0])
    hull_file = "%sconvex_hull_cluster%d.csv"%(prefix,i)
    hull_files.append([hull_file])
    csv.writer(open(hull_file,"wb")).writerows(hull_vertices)
    np.savetxt("%sconvex_hull_cluster%d.csv"%(prefix,i), hull_vertices, delimiter=",")

csv.writer(open("cluster_files.csv","wb")).writerows(cluster_files)
csv.writer(open("hull_files.csv","wb")).writerows(hull_files)
url = "http://localhost/Dengue/%s.php?prefix=%s&c="+str(len(clusters))
webbrowser.open_new(url%("polygons", prefix))
webbrowser.open_new_tab(url%("polygons_with_markers", prefix))
plt.show()
